{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "第二课pytorch",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM7IqvcJcLZTm8NJOXb896/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ligemlp/mylesson/blob/master/%E7%AC%AC%E4%BA%8C%E8%AF%BEpytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPCcQ3r6BAET",
        "colab_type": "code",
        "outputId": "a659cb15-d445-4de0-effc-807260c60cdb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "!git clone https://github.com/ligemlp/mylesson.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'mylesson'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Total 9 (delta 0), reused 0 (delta 0), pack-reused 9\u001b[K\n",
            "Unpacking objects: 100% (9/9), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgSyxpL8BDHM",
        "colab_type": "code",
        "outputId": "66c05031-f12c-479c-a1f6-cc52d41cece8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "!ls -R"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".:\n",
            "mylesson  sample_data\n",
            "\n",
            "./mylesson:\n",
            "text8  text8.zip\n",
            "\n",
            "./mylesson/text8:\n",
            "text8.dev.txt  text8.test.txt  text8.train.txt\n",
            "\n",
            "./sample_data:\n",
            "anscombe.json\t\t      mnist_test.csv\n",
            "california_housing_test.csv   mnist_train_small.csv\n",
            "california_housing_train.csv  README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZlO1bkxBc1T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import sklearn\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BU8QgK5rCQsv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "random.seed(1)\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "if USE_CUDA:\n",
        "    torch.cuda.manual_seed(1)\n",
        "#hyper paramters\n",
        "C = 3 #context window\n",
        "K = 100 #number of negative samples\n",
        "NUM_EPOCHS = 2\n",
        "MAX_VOCAB_SIZE = 30000\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.2\n",
        "EMBEDDING_SIZE = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3M_TsgkBDMle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_tokenize(text):\n",
        "    return text.split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWUmyL9okkme",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8456e233-e702-43f0-f5fd-cd36dc3fa98a"
      },
      "source": [
        "with open(\"./mylesson/text8/text8.train.txt\",\"r\") as f:\n",
        "    data = f.read()\n",
        "text = data.split()\n",
        "vocab = dict(Counter(text).most_common(MAX_VOCAB_SIZE - 1))\n",
        "vocab[\"<unk>\"] = len(text) - np.sum(list(vocab.values()))\n",
        "\n",
        "idx_to_word = [word for word in vocab.keys()]\n",
        "word_to_idx = {word:i for i,word in enumerate(idx_to_word)}\n",
        "\n",
        "word_counts = np.array([count for count in vocab.values()], dtype=np.float32)\n",
        "word_freq = word_counts / np.sum(word_counts)\n",
        "word_freq = word_freq ** (3./4.)\n",
        "word_freq = word_counts / np.sum(word_counts)\n",
        "VOCAB_SIZE = len(idx_to_word)\n",
        "VOCAB_SIZE"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sg1-Xx2Db5w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WordEmbedingDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, text, word_to_idx, idx_to_word, word_freq, word_counts):\n",
        "        super(WordEmbedingDataset, self).__init__()\n",
        "        self.text_encoded = [word_to_idx.get(word, word_to_idx[\"<unk>\"]) for word in text]\n",
        "        self.text_encoded = torch.LongTensor(self.text_encoded)\n",
        "        self.word_to_idx = word_to_idx\n",
        "        self.idx_to_word = idx_to_word\n",
        "        self.word_freq = torch.Tensor(word_freq)\n",
        "        self.word_counts = torch.Tensor(word_counts)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_encoded)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        center_word = self.text_encoded[idx]\n",
        "        pos_indices = list(range(idx - C,idx)) + list(range(idx+1, idx+C+1))\n",
        "        pos_indices = [i % len(self.text_encoded) for i in pos_indices]\n",
        "        pos_words = self.text_encoded[pos_indices]\n",
        "        neg_words = torch.multinomial(self.word_freq,  K * pos_words.shape[0], True)\n",
        "        return center_word, pos_words, neg_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz-XnIBCLnmW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8fc5e40c-ce44-4a8c-d568-cd6e90cdbcf0"
      },
      "source": [
        "dataset = WordEmbedingDataset(text, word_to_idx, idx_to_word, word_freq, word_counts)\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
        "center_word, pos_words, neg_words = dataset.__getitem__(100)\n",
        "pos_words"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([  58,   25, 6525,    1,  152,   32])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNqzckh0O4IN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EmbeddingModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size):\n",
        "        super(EmbeddingModel, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size)\n",
        "        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size)\n",
        "\n",
        "    def forward(self, input_labels, pos_labels, neg_labels):\n",
        "        input_embedding = self.in_embed(input_labels)#[batch_size, embed_size]\n",
        "        pos_embedding = self.out_embed(pos_labels)#[batch_size,(window_size*2), embed_size]\n",
        "        neg_embedding = self.out_embed(neg_labels)#[batch_size,(window_size*2*K),embed_size]\n",
        "\n",
        "        input_embedding = input_embedding.unsqueeze(2)#[batch_size,embed_size,1]\n",
        "        pos_dot = torch.bmm(pos_embedding, input_embedding).squeeze(2)#[batch_size, (window_size*2)]\n",
        "        neg_dot = torch.bmm(neg_embedding, -input_embedding).squeeze(2)#[batch_size, (window_size*2*K)]\n",
        "\n",
        "        log_pos = F.logsigmoid(pos_dot).sum(1)\n",
        "        log_neg = F.logsigmoid(neg_dot).sum(1)\n",
        "\n",
        "        loss = log_pos + log_neg\n",
        "\n",
        "        return -loss\n",
        "    \n",
        "    def input_embeddings(self):\n",
        "        return self.in_embed.weight.data.cpu().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCdXHFnSrhqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = EmbeddingModel(VOCAB_SIZE, EMBEDDING_SIZE)\n",
        "if USE_CUDA:\n",
        "    model = model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2WfwCSZrt4z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9c7a6646-87e7-447c-c606-4c029df09874"
      },
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(),lr=LEARNING_RATE)\n",
        "for e in range(NUM_EPOCHS):\n",
        "    for i , (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
        "        if USE_CUDA:\n",
        "            input_labels = input_labels.long()\n",
        "            pos_labels = pos_labels.long()\n",
        "            neg_labels = neg_labels.long()\n",
        "\n",
        "            input_labels = input_labels.cuda()\n",
        "            pos_labels = pos_labels.cuda()\n",
        "            neg_labels = neg_labels.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            loss = model(input_labels, pos_labels, neg_labels).mean()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                print(\"Epoch\",e,\"iteration\",i,loss.item())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 iteration 0 2515.32373046875\n",
            "Epoch 0 iteration 100 869.727294921875\n",
            "Epoch 0 iteration 200 574.3082275390625\n",
            "Epoch 0 iteration 300 616.73486328125\n",
            "Epoch 0 iteration 400 438.3895568847656\n",
            "Epoch 0 iteration 500 465.9903869628906\n",
            "Epoch 0 iteration 600 350.5777893066406\n",
            "Epoch 0 iteration 700 434.18865966796875\n",
            "Epoch 0 iteration 800 306.4776611328125\n",
            "Epoch 0 iteration 900 272.8446960449219\n",
            "Epoch 0 iteration 1000 257.8622741699219\n",
            "Epoch 0 iteration 1100 283.87042236328125\n",
            "Epoch 0 iteration 1200 215.76821899414062\n",
            "Epoch 0 iteration 1300 172.3601837158203\n",
            "Epoch 0 iteration 1400 233.6432342529297\n",
            "Epoch 0 iteration 1500 236.304443359375\n",
            "Epoch 0 iteration 1600 195.4098358154297\n",
            "Epoch 0 iteration 1700 162.46824645996094\n",
            "Epoch 0 iteration 1800 189.8173370361328\n",
            "Epoch 0 iteration 1900 172.59027099609375\n",
            "Epoch 0 iteration 2000 166.48712158203125\n",
            "Epoch 0 iteration 2100 203.97393798828125\n",
            "Epoch 0 iteration 2200 140.853271484375\n",
            "Epoch 0 iteration 2300 182.30224609375\n",
            "Epoch 0 iteration 2400 142.02601623535156\n",
            "Epoch 0 iteration 2500 145.9976043701172\n",
            "Epoch 0 iteration 2600 137.4312286376953\n",
            "Epoch 0 iteration 2700 180.14089965820312\n",
            "Epoch 0 iteration 2800 199.25436401367188\n",
            "Epoch 0 iteration 2900 125.66092681884766\n",
            "Epoch 0 iteration 3000 146.55026245117188\n",
            "Epoch 0 iteration 3100 141.55325317382812\n",
            "Epoch 0 iteration 3200 113.08348083496094\n",
            "Epoch 0 iteration 3300 129.07827758789062\n",
            "Epoch 0 iteration 3400 134.0078887939453\n",
            "Epoch 0 iteration 3500 116.50076293945312\n",
            "Epoch 0 iteration 3600 130.7536163330078\n",
            "Epoch 0 iteration 3700 115.39190673828125\n",
            "Epoch 0 iteration 3800 171.3262176513672\n",
            "Epoch 0 iteration 3900 148.61874389648438\n",
            "Epoch 0 iteration 4000 110.68608093261719\n",
            "Epoch 0 iteration 4100 112.99897766113281\n",
            "Epoch 0 iteration 4200 92.33787536621094\n",
            "Epoch 0 iteration 4300 106.45454406738281\n",
            "Epoch 0 iteration 4400 86.86697387695312\n",
            "Epoch 0 iteration 4500 110.17514038085938\n",
            "Epoch 0 iteration 4600 78.26362609863281\n",
            "Epoch 0 iteration 4700 123.5953598022461\n",
            "Epoch 0 iteration 4800 86.10668182373047\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-b119b8179a08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mUSE_CUDA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0minput_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-2fcc554da614>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mpos_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mpos_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_encoded\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpos_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mpos_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_encoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mneg_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_freq\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mK\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpos_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcenter_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDnmposUvVaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}